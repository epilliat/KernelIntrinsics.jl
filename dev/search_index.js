var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Vectorized-Memory-Access","page":"API Reference","title":"Vectorized Memory Access","text":"","category":"section"},{"location":"api/#Basic-Operations","page":"API Reference","title":"Basic Operations","text":"","category":"section"},{"location":"api/#Dynamic-Alignment-Operations","page":"API Reference","title":"Dynamic Alignment Operations","text":"...","category":"section"},{"location":"api/#Memory-Ordering","page":"API Reference","title":"Memory Ordering","text":"","category":"section"},{"location":"api/#Macros","page":"API Reference","title":"Macros","text":"","category":"section"},{"location":"api/#Scopes","page":"API Reference","title":"Scopes","text":"","category":"section"},{"location":"api/#Orderings","page":"API Reference","title":"Orderings","text":"","category":"section"},{"location":"api/#Warp-Operations","page":"API Reference","title":"Warp Operations","text":"","category":"section"},{"location":"api/#Macros-2","page":"API Reference","title":"Macros","text":"","category":"section"},{"location":"api/#Shuffle-Directions","page":"API Reference","title":"Shuffle Directions","text":"","category":"section"},{"location":"api/#Vote-Modes","page":"API Reference","title":"Vote Modes","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#KernelIntrinsics.vload","page":"API Reference","title":"KernelIntrinsics.vload","text":"vload(A::AbstractArray{T}, idx, ::Val{Nitem}, ::Val{Rebase}=Val(true)) -> NTuple{Nitem,T}\n\nLoad Nitem elements from array A as a tuple, using vectorized memory operations on GPU. Nitem must be a positive power of 2.\n\nArguments\n\nA: Source array\nidx: Starting index\nNitem: Number of elements to load (must be a positive power of 2)\nRebase: Indexing mode (default: Val(true))\n\nIndexing Modes\n\nVal(true) (rebased): Uses 1-based block indexing — idx selects the idx-th contiguous block of Nitem elements, i.e. loads from (idx-1)*Nitem + 1 to idx*Nitem. For example, idx=2 loads elements [5,6,7,8] for Nitem=4. When the array base pointer is Nitem-aligned, this generates optimal aligned vector loads (ld.global.v4); otherwise falls back to vload_multi.\nVal(false) (direct): Loads starting directly at idx, so idx=2 loads elements [2,3,4,5]. Always uses vload_multi to handle potential misalignment.\n\nExample\n\na = CuArray{Int32}(1:16)\n\n# Rebased indexing (default): idx=2 → loads block 2, i.e. elements 5,6,7,8\nvalues = vload(a, 2, Val(4))  # returns (5, 6, 7, 8)\n\n# Direct indexing: idx=2 → loads elements 2,3,4,5\nvalues = vload(a, 2, Val(4), Val(false))  # returns (2, 3, 4, 5)\n\nSee also: vstore!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.vstore!","page":"API Reference","title":"KernelIntrinsics.vstore!","text":"vstore!(A::AbstractArray{T}, idx, values::NTuple{Nitem,T}, ::Val{Rebase}=Val(true)) -> Nothing\n\nStore Nitem elements from a tuple to array A, using vectorized memory operations on GPU. Nitem must be a positive power of 2.\n\nArguments\n\nA: Destination array\nidx: Starting index\nvalues: Tuple of Nitem elements to store\nRebase: Indexing mode (default: Val(true))\n\nIndexing Modes\n\nVal(true) (rebased): Uses 1-based block indexing — idx selects the idx-th contiguous block of Nitem elements, i.e. stores to (idx-1)*Nitem + 1 through idx*Nitem. For example, idx=2 stores to elements [5,6,7,8] for Nitem=4. When the array base pointer is Nitem-aligned, this generates optimal aligned vector stores (st.global.v4); otherwise falls back to vstore_multi!.\nVal(false) (direct): Stores starting directly at idx, so idx=2 stores to elements [2,3,4,5]. Always uses vstore_multi! to handle potential misalignment.\n\nExample\n\nb = CUDA.zeros(Int32, 16)\n\n# Rebased indexing (default): idx=2 → stores to block 2, i.e. elements 5,6,7,8\nvstore!(b, 2, (Int32(10), Int32(20), Int32(30), Int32(40)))\n\n# Direct indexing: idx=2 → stores to elements 2,3,4,5\nvstore!(b, 2, (Int32(10), Int32(20), Int32(30), Int32(40)), Val(false))\n\nSee also: vload\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.vload_multi","page":"API Reference","title":"KernelIntrinsics.vload_multi","text":"vload_multi(A::AbstractArray{T}, i, ::Val{Nitem}) -> NTuple{Nitem,T}\n\nLoad Nitem elements from array A starting at index i, automatically handling arbitrary alignment. Nitem must be a positive power of 2.\n\nComputes alignment at runtime as mod = (base_ptr_in_elements + (i - 1)) % Nitem + 1, where base_ptr_in_elements = pointer(A) ÷ sizeof(T), and dispatches to a statically-compiled load pattern via a switch table. This generates a mix of ld.global.v4, ld.global.v2, and scalar loads to maximize throughput.\n\nExample\n\nsrc = cu(Int32.(1:100))\n\n# Works for any starting index — alignment handled automatically\nvalues = vload_multi(src, 7, Val(8))  # loads elements 7:14\n\nSee also: vload, vstore_multi!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.vstore_multi!","page":"API Reference","title":"KernelIntrinsics.vstore_multi!","text":"vstore_multi!(A::AbstractArray{T}, i, values::NTuple{Nitem,T}) -> Nothing\n\nStore Nitem elements to array A starting at index i, automatically handling arbitrary alignment. Nitem must be a positive power of 2.\n\nComputes alignment at runtime as mod = (base_ptr_in_elements + (i - 1)) % Nitem + 1, where base_ptr_in_elements = pointer(A) ÷ sizeof(T), and dispatches to a statically-compiled store pattern via a switch table. This generates a mix of st.global.v4, st.global.v2, and scalar stores to maximize throughput.\n\nExample\n\ndst = cu(zeros(Int32, 100))\n\n# Works for any starting index — alignment handled automatically\nvstore_multi!(dst, 7, (Int32(1), Int32(2), Int32(3), Int32(4)))\n\nSee also: vstore!, vload_multi\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.@fence","page":"API Reference","title":"KernelIntrinsics.@fence","text":"@fence [Scope] [Ordering]\n\nInsert a memory fence with specified scope and ordering.\n\nA memory fence ensures that memory operations before the fence are visible to other threads before operations after the fence. This is essential for correct synchronization in parallel GPU code.\n\nArguments\n\nScope (optional): Visibility scope, one of Device (default, maps to .gpu in PTX), Workgroup (maps to .cta), or System (maps to .sys).\nOrdering (optional): Memory ordering, one of Acquire, Release, AcqRel (default), or SeqCst. Weak, Volatile, and Relaxed are not valid for fences.\n\nArguments can be specified in any order.\n\nGenerated PTX\n\n@fence → fence.acq_rel.gpu\n@fence Workgroup → fence.acq_rel.cta\n@fence System SeqCst → fence.sc.sys\n\nExample\n\n@kernel function synchronized_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible to other threads before continuing\n    Flag[1] = 1\nend\n\n# Explicit scope and ordering\n@fence Device AcqRel\n@fence Workgroup Release\n@fence System SeqCst\n@fence SeqCst Device  # Order doesn't matter\n\nSee also: @access\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@access","page":"API Reference","title":"KernelIntrinsics.@access","text":"@access [Scope] [Ordering] expr\n\nPerform a memory load or store with specified scope and ordering semantics.\n\nThis macro provides fine-grained control over memory ordering for lock-free synchronization patterns on GPU. It generates appropriate ld.acquire or st.release PTX instructions.\n\nArguments\n\nScope (optional): Visibility scope, one of Device (default), Workgroup, or System. Cannot be specified with Volatile or Weak orderings, as those are scope-less.\nOrdering (optional): Memory ordering (see below).\nexpr: A load or store expression (see Syntax Forms).\n\nArguments can be specified in any order.\n\nOrderings\n\nFor loads (default: Acquire):\n\nAcquire: Subsequent reads see all writes before the corresponding release.\nRelaxed: No ordering guarantees.\nVolatile: Volatile load — bypasses cache, scope-less.\nWeak: Weak load — scope-less.\n\nFor stores (default: Release):\n\nRelease: Prior writes are visible to other threads before this store.\nRelaxed: No ordering guarantees.\nVolatile: Volatile store — bypasses cache, scope-less.\nWeak: Weak store — scope-less.\n\nAcqRel and SeqCst are not valid for individual loads/stores; use @fence instead.\n\nSyntax Forms\n\n@access array[idx] = value                  # Release store (default)\n@access var = array[idx]                    # Acquire load, result bound to var (default)\n@access array[idx]                          # Acquire load, result returned directly\n\n@access Release array[idx] = value          # Explicit ordering\n@access Acquire var = array[idx]            # Explicit ordering\n@access Device Release array[idx] = value   # Explicit scope and ordering\n@access SeqCst Device  # Order doesn't matter\n\nExample\n\n@kernel function producer_consumer(X, Flag)\n    if @index(Global, Linear) == 1\n        X[1] = 42\n        @access Flag[1] = 1  # Release store: X[1]=42 visible before Flag[1]=1\n    end\n\n    # Other threads spin-wait using standalone load form\n    while (@access Acquire Flag[1]) != 1\n    end\n    # Now X[1] is guaranteed to be 42\nend\n\nSee also: @fence\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.Scope","page":"API Reference","title":"KernelIntrinsics.Scope","text":"Scope\n\nAbstract type representing the scope of a memory operation or fence.\n\nSubtypes:\n\nWorkgroup: Thread block/workgroup scope\nDevice: Device/GPU scope\nSystem: System scope (includes CPU and other devices)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Workgroup","page":"API Reference","title":"KernelIntrinsics.Workgroup","text":"Workgroup <: Scope\n\nThread block/workgroup scope. Synchronizes memory operations within a single thread block/workgroup.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Device","page":"API Reference","title":"KernelIntrinsics.Device","text":"Device <: Scope\n\nDevice/GPU scope. Synchronizes memory operations across all thread blocks on a single device.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.System","page":"API Reference","title":"KernelIntrinsics.System","text":"System <: Scope\n\nSystem scope. Synchronizes memory operations across all devices, including the CPU.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Ordering","page":"API Reference","title":"KernelIntrinsics.Ordering","text":"Ordering\n\nAbstract type representing memory ordering semantics.\n\nSubtypes:\n\nWeak: Minimal ordering guarantees\nVolatile: Prevents compiler caching/reordering; no atomicity implied\nRelaxed: Atomicity only, no synchronization\nAcquire: Acquire semantics\nRelease: Release semantics\nAcqRel: Acquire-Release semantics\nSeqCst: Sequential consistency\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Weak","page":"API Reference","title":"KernelIntrinsics.Weak","text":"Weak <: Ordering\n\nWeak memory ordering. Provides minimal ordering guarantees, allowing maximum hardware and compiler reordering flexibility.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Volatile","page":"API Reference","title":"KernelIntrinsics.Volatile","text":"Volatile <: Ordering\n\nVolatile memory ordering. Prevents the compiler from caching or reordering the operation, ensuring each access goes directly to memory. Does not imply atomicity or inter-thread synchronization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Relaxed","page":"API Reference","title":"KernelIntrinsics.Relaxed","text":"Relaxed <: Ordering\n\nRelaxed memory ordering. Ensures atomicity of individual operations but provides no synchronization guarantees. Operations may be reordered freely by hardware and compiler.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Acquire","page":"API Reference","title":"KernelIntrinsics.Acquire","text":"Acquire <: Ordering\n\nAcquire memory ordering. Ensures that all memory operations after this point see all writes that happened before a corresponding Release operation.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Release","page":"API Reference","title":"KernelIntrinsics.Release","text":"Release <: Ordering\n\nRelease memory ordering. Ensures that all memory operations before this point are visible to threads that subsequently perform an Acquire operation.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.AcqRel","page":"API Reference","title":"KernelIntrinsics.AcqRel","text":"AcqRel <: Ordering\n\nAcquire-Release memory ordering. Combines both Acquire and Release semantics. Used for read-modify-write operations and fences.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.SeqCst","page":"API Reference","title":"KernelIntrinsics.SeqCst","text":"SeqCst <: Ordering\n\nSequential consistency. Provides the strongest memory ordering guarantees, establishing a total order of all sequentially consistent operations across all threads.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.@warpsize","page":"API Reference","title":"KernelIntrinsics.@warpsize","text":"@warpsize()\n\nReturn the warp size of the current backend as an Int. Queries the backend at runtime — 32 on CUDA, but may differ on future backends.\n\nSee also: @shfl, @warpreduce, @warpfold\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@shfl","page":"API Reference","title":"KernelIntrinsics.@shfl","text":"@shfl(direction, val, src, [warpsize=@warpsize()], [mask=0xffffffff])\n\nPerform a warp shuffle operation, exchanging values between lanes within a warp.\n\nThe default warpsize is retrieved at runtime via @warpsize(), which queries the backend (32 on CUDA, but may differ on future backends).\n\nArguments\n\ndirection: Shuffle direction (Up, Down, Xor, or Idx)\nval: Value to shuffle (supports primitives, structs, and NTuples)\nsrc: Offset (for Up/Down), XOR mask (for Xor), or source lane 0-based index (for Idx)\nwarpsize: Warp size (default: @warpsize())\nmask: Lane participation mask (default: 0xffffffff for all lanes)\n\nExample\n\n@kernel function shfl_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n\n    shuffled = @shfl(Up, val, 1)    # Lane i receives from lane i-1; lane 0 keeps its value\n    shuffled = @shfl(Down, val, 1)  # Lane i receives from lane i+1; last lane keeps its value\n    shuffled = @shfl(Xor, val, 1)   # Swap adjacent pairs (lane 0↔1, 2↔3, ...)\n    shuffled = @shfl(Idx, val, 0)   # Broadcast lane 0 to all lanes\n\n    dst[I] = shuffled\nend\n\nSee also: @warpreduce, @warpfold\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@warpreduce","page":"API Reference","title":"KernelIntrinsics.@warpreduce","text":"@warpreduce(val, lane, [op=+], [warpsize=@warpsize()], [mask=0xffffffff])\n\nPerform an inclusive prefix scan within a warp using shuffle-up operations.\n\nAfter this macro, lane i (1-based) holds the result of applying op to the values of lanes 1 through i. The result in the last lane is the warp-wide reduction.\n\nThe default warpsize is retrieved at runtime via @warpsize(), which queries the backend (32 on CUDA, but may differ on future backends).\n\nArguments\n\nval: Value to scan (modified in-place)\nlane: Current lane index (1-based)\nop: Binary associative operator (default: +)\nwarpsize: Warp size (default: @warpsize())\nmask: Lane participation mask (default: 0xffffffff)\n\nExample\n\n@kernel function scan_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % @warpsize() + 1\n\n    @warpreduce(val, lane)\n\n    dst[I] = val\nend\n\n# Input:  [1, 2, 3, 4, ..., 32]\n# Output: [1, 3, 6, 10, ..., 528]\n\nSee also: @warpfold, @shfl\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@warpfold","page":"API Reference","title":"KernelIntrinsics.@warpfold","text":"@warpfold(val, lane, [op=+], [warpsize=@warpsize()], [mask=0xffffffff])\n\nPerform a warp-wide reduction, combining all lane values using the specified operator. Uses shuffle-down operations internally. After this macro, all lanes hold the warp-wide result.\n\nThe default warpsize is retrieved at runtime via @warpsize(), which queries the backend (32 on CUDA, but may differ on future backends).\n\nArguments\n\nval: Value to reduce (modified in-place)\nlane: Current lane index (1-based; accepted for API consistency but unused)\nop: Binary associative operator (default: +)\nwarpsize: Warp size (default: @warpsize())\nmask: Lane participation mask (default: 0xffffffff)\n\nExample\n\n@kernel function reduce_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % @warpsize() + 1  # required by API, not used internally\n\n    @warpfold(val, lane)\n\n    if lane == 1\n        dst[1] = val  # Contains sum of all warp values\n    end\nend\n\n# Input:  [1, 2, 3, ..., 32]\n# Output: dst[1] = 528\n\nSee also: @warpreduce, @shfl\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@vote","page":"API Reference","title":"KernelIntrinsics.@vote","text":"@vote(mode, predicate, [mask=0xffffffff])\n\nPerform a warp vote operation, evaluating a predicate across all participating lanes.\n\nArguments\n\nmode: Vote mode (All, AnyLane, Uni, or Ballot)\npredicate: Boolean predicate to evaluate\nmask: Lane participation mask (default: 0xffffffff for all lanes)\n\nExample\n\n@kernel function vote_kernel(dst, src, threshold)\n    I = @index(Global, Linear)\n    val = src[I]\n\n    all_above = @vote(All,     val > threshold)  # true if all lanes satisfy predicate\n    any_above = @vote(AnyLane, val > threshold)  # true if any lane satisfies predicate\n    uniform   = @vote(Uni,     val > threshold)  # true if all lanes have the same result\n    bits      = @vote(Ballot,  val > threshold)  # UInt32 bitmask: bit i (0-based) set if lane i satisfies predicate\n\n    dst[I] = bits\nend\n\nSee also: @shfl\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.Direction","page":"API Reference","title":"KernelIntrinsics.Direction","text":"Direction\n\nAbstract type representing warp shuffle directions.\n\nSubtypes:\n\nUp: Shuffle values from lower lane indices\nDown: Shuffle values from higher lane indices\nXor: Shuffle values using XOR of lane indices\nIdx: Shuffle values from a specific lane index\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Up","page":"API Reference","title":"KernelIntrinsics.Up","text":"Up <: Direction\n\nShuffle direction where each lane receives a value from a lane with a lower index.\n\n@shfl(Up, val, offset): Lane i receives the value from lane i - offset. Lanes where i < offset keep their original value.\n\nResult for offset=1 (warpsize=32): [1, 1, 2, 3, 4, ..., 31]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Down","page":"API Reference","title":"KernelIntrinsics.Down","text":"Down <: Direction\n\nShuffle direction where each lane receives a value from a lane with a higher index.\n\n@shfl(Down, val, offset): Lane i receives the value from lane i + offset. Lanes where i + offset >= warpsize keep their original value.\n\nResult for offset=1 (warpsize=32): [2, 3, 4, ..., 31, 32, 32]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Xor","page":"API Reference","title":"KernelIntrinsics.Xor","text":"Xor <: Direction\n\nShuffle direction where each lane exchanges values based on XOR of lane indices.\n\n@shfl(Xor, val, mask): Lane i receives the value from lane i ⊻ mask.\n\nCommon patterns:\n\nmask=1: Swap adjacent pairs (0↔1, 2↔3, ...)\nmask=16: Swap first and second half of warp\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Idx","page":"API Reference","title":"KernelIntrinsics.Idx","text":"Idx <: Direction\n\nShuffle direction where all lanes receive a value from a specific lane index.\n\n@shfl(Idx, val, lane): All lanes receive the value from lane lane (0-based).\n\nUseful for broadcasting a value from one lane to all others.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Mode","page":"API Reference","title":"KernelIntrinsics.Mode","text":"Mode\n\nAbstract type representing warp vote modes.\n\nSubtypes:\n\nAll: True if predicate is true for all lanes\nAnyLane: True if predicate is true for any lane\nUni: True if predicate is uniform across all lanes\nBallot: Returns a bitmask of predicate values\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.All","page":"API Reference","title":"KernelIntrinsics.All","text":"All <: Mode\n\nVote mode that returns true if the predicate is true for all participating lanes.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.AnyLane","page":"API Reference","title":"KernelIntrinsics.AnyLane","text":"AnyLane <: Mode\n\nVote mode that returns true if the predicate is true for any participating lane.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Uni","page":"API Reference","title":"KernelIntrinsics.Uni","text":"Uni <: Mode\n\nVote mode that returns true if the predicate has the same value across all participating lanes.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelIntrinsics.Ballot","page":"API Reference","title":"KernelIntrinsics.Ballot","text":"Ballot <: Mode\n\nVote mode that returns a UInt32 bitmask where bit i (0-based) is set if lane i's predicate is true.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Memory-Fences","page":"Examples","title":"Memory Fences","text":"Use @fence to ensure memory operations are visible across threads before proceeding:\n\nusing KernelIntrinsics\nusing KernelAbstractions, CUDA\n\n@kernel function fence_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible to all threads before next operations\n    Flag[1] = 1\nend\n\nX = cu([1])\nFlag = cu([0])\nfence_kernel(CUDABackend())(X, Flag; ndrange=1)\n\nThe @fence macro generates fence.acq_rel.gpu instructions in PTX assembly, ensuring proper memory ordering across the GPU.","category":"section"},{"location":"examples/#Ordered-Memory-Access","page":"Examples","title":"Ordered Memory Access","text":"The @access macro provides acquire/release semantics for fine-grained memory ordering:\n\n@kernel function access_kernel(X, Flag)\n    if @index(Global, Linear) == 1\n        X[1] = 10\n        @access Flag[1] = 1  # Release store\n    end\n\n    # Other threads wait for Flag[1] == 1\n    while (@access Acquire Flag[1]) != 1  # Acquire load\n    end\n\n    # Safely use X[1] here\nend\n\nX = cu([i for i in 1:1000])\nFlag = cu([0])\naccess_kernel(CUDABackend())(X, Flag; ndrange=1000)\n\nThis generates st.release.gpu and ld.acquire.gpu instructions, providing lock-free synchronization patterns.","category":"section"},{"location":"examples/#Warp-Operations","page":"Examples","title":"Warp Operations","text":"","category":"section"},{"location":"examples/#Shuffle-Operations","page":"Examples","title":"Shuffle Operations","text":"Exchange values between threads within a warp:\n\n@kernel function shfl_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    offset = 1\n    shuffled_val = @shfl(Up, val, offset)  # Default: warpsize=32, full mask\n    dst[I] = shuffled_val\nend\n\nsrc = cu(Int32.(1:32))\ndst = cu(zeros(Int32, 32))\nshfl_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst = [1, 1, 2, 3, 4, ..., 31]","category":"section"},{"location":"examples/#Shuffle-with-Custom-Types","page":"Examples","title":"Shuffle with Custom Types","text":"Unlike CUDA.jl, KernelIntrinsics.jl supports shuffle operations on arbitrary user-defined bitstype structs, including nested and composite types, as well as NTuples:\n\nstruct Sub\n    a::Float16\n    b::UInt8\nend\n\nstruct ComplexType\n    x::Int32\n    y::Sub\n    z::Float64\nend\n\n@kernel function shfl_custom_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    offset = 1\n    shuffled_val = @shfl(Up, val, offset)\n    dst[I] = shuffled_val\nend\n\n# Nested structs\nsrc = cu([ComplexType(i, Sub(i, i), i) for i in 1:32])\ndst = cu([ComplexType(0, Sub(0, 0), 0) for i in 1:32])\nshfl_custom_kernel(CUDABackend())(dst, src; ndrange=32)\n\n# NTuples\nsrc = cu([(Int32(i), Int32(i + 100)) for i in 1:32])\ndst = cu([(Int32(0), Int32(0)) for _ in 1:32])\nshfl_custom_kernel(CUDABackend())(dst, src; ndrange=32)","category":"section"},{"location":"examples/#Warp-Reduce-(Inclusive-Scan)","page":"Examples","title":"Warp Reduce (Inclusive Scan)","text":"Perform inclusive prefix sum within a warp:\n\n@kernel function warpreduce_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % 32 + 1\n    @warpreduce(val, lane, +)\n    dst[I] = val\nend\n\nsrc = cu(Int32.(1:32))\ndst = cu(zeros(Int32, 32))\nwarpreduce_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst = [1, 3, 6, 10, ..., 528]  (cumulative sum)","category":"section"},{"location":"examples/#Warp-Fold-(Reduction)","page":"Examples","title":"Warp Fold (Reduction)","text":"Reduce all values in a warp to a single result:\n\n@kernel function warpfold_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % 32 + 1\n    @warpfold(val, lane, +)\n    dst[I] = val\nend\n\nsrc = cu(Int32.(1:32))\ndst = cu(zeros(Int32, 32))\nwarpfold_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst[1] = 528 (sum of 1:32), rest are undefined","category":"section"},{"location":"examples/#Vectorized-Memory-Access","page":"Examples","title":"Vectorized Memory Access","text":"","category":"section"},{"location":"examples/#Basic-Vectorized-Access","page":"Examples","title":"Basic Vectorized Access","text":"Use vload and vstore! for aligned vectorized operations:\n\n@kernel function vectorized_kernel(dst, src, i)\n    # Load 4 elements with rebase (i=2 → loads from index 5,6,7,8)\n    values = vload(src, i, Val(4), Val(true))\n\n    # Store 4 elements with rebase\n    vstore!(dst, i, values, Val(true))\nend\n\nsrc = cu(Int32.(1:32))\ndst = cu(zeros(Int32, 32))\nvectorized_kernel(CUDABackend())(dst, src, 2; ndrange=1)\n# dst[5:8] = [5, 6, 7, 8]\n\nThis generates efficient ld.global.v4 and st.global.v4 PTX instructions. The vector width depends on element type (v4 for Int32/Float32, v2 for Int64/Float64).","category":"section"},{"location":"examples/#Dynamic-Alignment-with-vload_multi-/-vstore_multi!","page":"Examples","title":"Dynamic Alignment with vload_multi / vstore_multi!","text":"When the starting index is not known at compile time, alignment cannot be guaranteed. vload_multi and vstore_multi! handle this by:\n\nComputing mod = (i - 1) % N + 1 at runtime (where N is the vector width)\nUsing a switch table to dispatch to the appropriate statically-compiled function with Val(mod)\nEmitting a mix of vectorized instructions to maximize throughput\n\n@kernel function dynamic_load_kernel(dst, src, i, ::Val{N}) where {N}\n    # i can be any runtime value — alignment handled automatically\n    values = vload_multi(src, i, Val(N))\n    for j in 1:N\n        dst[j] = values[j]\n    end\nend\n\nsrc = cu(Int32.(1:100))\ndst = cu(zeros(Int32, 16))\n\n# Works for any starting index\ndynamic_load_kernel(CUDABackend())(dst, src, 7, Val(16); ndrange=1)\n# dst = [7, 8, 9, ..., 22]\n\nThe generated PTX will contain a mix of ld.global.v4, ld.global.v2, and scalar loads depending on the runtime alignment, maximizing memory throughput while handling arbitrary offsets.\n\n@kernel function dynamic_store_kernel(dst, i)\n    values = (Int32(10), Int32(20), Int32(30), Int32(40))\n    vstore_multi!(dst, i, values)\nend\n\ndst = cu(zeros(Int32, 100))\ndynamic_store_kernel(CUDABackend())(dst, 3; ndrange=1)\n# dst[3:6] = [10, 20, 30, 40]","category":"section"},{"location":"examples/#Pattern-Based-Access","page":"Examples","title":"Pattern-Based Access","text":"For custom access patterns, use vload_pattern and vstore_pattern!:\n\n@kernel function pattern_kernel(dst, src, i)\n    # Pattern (1, 2, 1) means: load 1, then 2, then 1 element\n    values = vload_pattern(src, i, Val((1, 2, 1)))\n    vstore_pattern!(dst, i, values, Val((1, 2, 1)))\nend\n\nsrc = cu(Int32.(1:16))\ndst = cu(zeros(Int32, 16))\npattern_kernel(CUDABackend())(dst, src, 2; ndrange=1)\n# dst[2:5] = src[2:5]","category":"section"},{"location":"examples/#Inspecting-Generated-Code","page":"Examples","title":"Inspecting Generated Code","text":"You can verify the generated PTX assembly to confirm proper instruction generation:\n\nbuf = IOBuffer()\nCUDA.@device_code_ptx io = buf fence_kernel(CUDABackend())(X, Flag; ndrange=1)\nasm = String(take!(buf))\noccursin(\"fence.acq_rel.gpu\", asm)  # true\n\nExample: verifying vectorized instructions are generated:\n\n@kernel function test_vload_kernel(a, b, i)\n    y = vload(a, i, Val(4))\n    b[1] = sum(y)\nend\n\na = cu(Int32.(1:16))\nb = cu(zeros(Int32, 4))\n\nbuf = IOBuffer()\nCUDA.@device_code_ptx io = buf test_vload_kernel(CUDABackend())(a, b, 2; ndrange=1)\nasm = String(take!(buf))\noccursin(\"ld.global.v4\", asm)  # true","category":"section"},{"location":"#KernelIntrinsics.jl","page":"Home","title":"KernelIntrinsics.jl","text":"⚠️ Warning: This package provides low-level GPU primitives intended for library developers, not end users. If you're looking for high-level GPU programming in Julia, use CUDA.jl or KernelAbstractions.jl directly. KernelIntrinsics.jl is similar in scope to GPUArraysCore.jl — a building block for other packages.\n\n⚠️ Current Limitations:No bounds checking — out-of-bounds access will cause undefined behavior\nCUDA-only (other backends planned)\n\nA Julia package providing low-level memory access primitives and warp-level operations for GPU programming with KernelAbstractions.jl. KernelIntrinsics.jl enables fine-grained control over memory ordering, synchronization, and vectorized operations for high-performance GPU kernels.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Memory Fences: Explicit memory synchronization with @fence macro\nOrdered Memory Access: Acquire/release semantics with @access macro\nWarp Operations: Efficient intra-warp communication and reduction primitives\n@shfl: Warp shuffle operations (Up, Down, Xor, Idx modes)\n@warpreduce: Inclusive scan within a warp\n@warpfold: Warp-wide reduction to a single value\nVectorized Memory Operations: Hardware-accelerated vector loads and stores with vload, vstore!, vload_multi, and vstore_multi!. Array views (SubArray) are supported and fall back to scalar tuple operations automatically.","category":"section"},{"location":"#Cross-Architecture-Support","page":"Home","title":"Cross-Architecture Support","text":"Currently, KernelIntrinsics.jl is implemented exclusively for CUDA GPUs via the CUDABackend. The package leverages CUDA-specific PTX instructions for memory fences (fence.acq_rel.gpu), ordered memory access (ld.acquire.gpu, st.release.gpu), warp shuffle operations, and vectorized memory transactions.\n\nWhile the current implementation is CUDA-specific, the macro-based API is designed with portability in mind. Future releases may extend support to other GPU backends (AMD ROCm, Intel oneAPI, Apple Metal). Contributions to enable cross-platform support are welcome.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/YOURUSERNAME/KernelIntrinsics.jl\")","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KernelIntrinsics\nusing KernelAbstractions, CUDA\n\n@kernel function example_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible to all threads\n    @access Flag[1] = 1  # Release store\nend\n\nX = cu([1])\nFlag = cu([0])\nexample_kernel(CUDABackend())(X, Flag; ndrange=1)","category":"section"},{"location":"#Memory-Ordering-Semantics","page":"Home","title":"Memory Ordering Semantics","text":"@fence: Full acquire-release fence across all device threads\n@access Release: Ensures prior writes are visible before the store\n@access Acquire: Ensures subsequent reads see prior writes\n@access Acquire Device: Explicitly specifies device scope (default)","category":"section"},{"location":"#Performance-Considerations","page":"Home","title":"Performance Considerations","text":"Warp operations are most efficient when all threads in a warp participate\nVectorized operations can significantly improve memory bandwidth utilization\nUse the minimum required memory ordering (acquire/release over fences when possible)\nDefault warp size is 32; operations assume full warp participation with mask 0xffffffff\nvload_multi/vstore_multi! have a small runtime overhead for the alignment switch, but this is typically negligible compared to memory latency\nvload/vstore! on array views fall back to scalar tuple operations; performance-critical code should prefer contiguous arrays for vectorized instructions","category":"section"},{"location":"#Implementation-Notes","page":"Home","title":"Implementation Notes","text":"","category":"section"},{"location":"#Memory-Ordering-and-Scopes","page":"Home","title":"Memory Ordering and Scopes","text":"The implementation of memory fences, orderings, and scopes is inspired by UnsafeAtomics.jl. This package includes tests demonstrating that these primitives work correctly on CUDA, generating the expected PTX instructions (fence.acq_rel.gpu, ld.acquire.gpu, st.release.gpu, etc.).","category":"section"},{"location":"#Warp-Shuffle-Operations","page":"Home","title":"Warp Shuffle Operations","text":"The warp shuffle implementation builds upon CUDA.jl's approach but generalizes it to support any concrete bitstype struct, including nested and composite types, as well as NTuples. The backend only needs to implement 32-bit shuffle operations; larger types are automatically decomposed. Future backends that do not natively support 32-bit atomic operations may require additional handling.","category":"section"},{"location":"#Vectorized-Memory-Access","page":"Home","title":"Vectorized Memory Access","text":"Vectorized loads and stores (vload, vstore!, vload_multi, vstore_multi!) use LLVM intrinsic functions to generate efficient vector instructions (ld.global.v4, st.global.v4) on contiguous arrays. Array views (SubArray) are fully supported via an automatic fallback to scalar tuple operations, ensuring correctness at the cost of vectorization.\n\nCurrent limitations:\n\nNo bounds checking\nFuture versions may add vectorized support for non-contiguous views","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10+\nKernelAbstractions.jl\nCUDA.jl (for CUDA backend)","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"api.md\", \"examples.md\"]\nDepth = 2","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"}]
}
