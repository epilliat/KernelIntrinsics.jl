var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Vectorized-Memory-Access","page":"API Reference","title":"Vectorized Memory Access","text":"","category":"section"},{"location":"api/#Memory-Ordering","page":"API Reference","title":"Memory Ordering","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#KernelIntrinsics.vload","page":"API Reference","title":"KernelIntrinsics.vload","text":"vload(A::AbstractArray{T}, idx, ::Val{Nitem}, ::Val{Rebase}=Val(true)) -> NTuple{Nitem,T}\n\nLoad Nitem elements from array A as a tuple, using vectorized memory operations on GPU.\n\nArguments\n\nA: Source array\nidx: Starting index\nNitem: Number of elements to load (must be a power of 2)\nRebase: Indexing mode (default: Val(true))\n\nIndexing Modes\n\nVal(true) (rebased): Index is multiplied by Nitem, so idx=2 loads elements [5,6,7,8] for Nitem=4. This mode generates optimal aligned vector loads (ld.global.v4).\nVal(false) (direct): Loads starting directly at idx, so idx=2 loads elements [2,3,4,5]. Handles misaligned access by decomposing into smaller aligned loads.\n\nExample\n\na = CuArray{Int32}(1:16)\n\n# Rebased indexing (default): idx=2 → loads elements 5,6,7,8\nvalues = vload(a, 2, Val(4))  # returns (5, 6, 7, 8)\n\n# Direct indexing: idx=2 → loads elements 2,3,4,5\nvalues = vload(a, 2, Val(4), Val(false))  # returns (2, 3, 4, 5)\n\nSee also: vstore!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.vstore!","page":"API Reference","title":"KernelIntrinsics.vstore!","text":"vstore!(A::AbstractArray{T}, idx, values::NTuple{Nitem,T}, ::Val{Rebase}=Val(true)) -> Nothing\n\nStore Nitem elements from a tuple to array A, using vectorized memory operations on GPU.\n\nArguments\n\nA: Destination array\nidx: Starting index\nvalues: Tuple of Nitem elements to store\nRebase: Indexing mode (default: Val(true))\n\nIndexing Modes\n\nVal(true) (rebased): Index is multiplied by Nitem, so idx=2 stores to elements [5,6,7,8] for Nitem=4. This mode generates optimal aligned vector stores (st.global.v4).\nVal(false) (direct): Stores starting directly at idx, so idx=2 stores to elements [2,3,4,5]. Handles misaligned access by decomposing into smaller aligned stores.\n\nExample\n\nb = CUDA.zeros(Int32, 16)\n\n# Rebased indexing (default): idx=2 → stores to elements 5,6,7,8\nvstore!(b, 2, (Int32(10), Int32(20), Int32(30), Int32(40)))\n\n# Direct indexing: idx=2 → stores to elements 2,3,4,5\nvstore!(b, 2, (Int32(10), Int32(20), Int32(30), Int32(40)), Val(false))\n\nSee also: vload\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelIntrinsics.@access","page":"API Reference","title":"KernelIntrinsics.@access","text":"@access [Scope] [Ordering] expr\n\nPerform a memory load or store with specified scope and ordering semantics.\n\nThis macro provides fine-grained control over memory ordering for lock-free synchronization patterns on GPU. It generates appropriate ld.acquire or st.release PTX instructions.\n\nArguments\n\nScope (optional): Visibility scope, one of Device (default), Workgroup, or System\nOrdering (optional): Memory ordering (see below)\nexpr: Either a load (var = array[idx]) or store (array[idx] = value) expression\n\nOrderings\n\nFor loads (default: Acquire):\n\nAcquire: Subsequent reads see all writes before the corresponding release\nRelaxed: No ordering guarantees\nVolatile: Volatile load (scope-less)\nWeak: Weak load (scope-less)\n\nFor stores (default: Release):\n\nRelease: Prior writes are visible before this store\nRelaxed: No ordering guarantees\nVolatile: Volatile store (scope-less)\nWeak: Weak store (scope-less)\n\nAcqRel and SeqCst are not valid for individual loads/stores (use @fence instead). Volatile and Weak cannot have an explicit scope.\n\nSyntax Forms\n\n@access array[idx] = value          # Release store (default)\n@access var = array[idx]            # Acquire load (default)\n@access array[idx]                  # Acquire load, returns value directly\n\n@access Release array[idx] = value  # Explicit ordering\n@access Acquire var = array[idx]    # Explicit ordering\n@access Device Release array[idx] = value  # Explicit scope and ordering\n\nExample\n\n@kernel function producer_consumer(X, Flag)\n    if @index(Global, Linear) == 1\n        X[1] = 42\n        @access Flag[1] = 1  # Release store: X[1]=42 visible before Flag[1]=1\n    end\n\n    # Other threads wait\n    while (@access Acquire Flag[1]) != 1\n    end\n    # Now X[1] is guaranteed to be 42\nend\n\nSee also: @fence\n\n\n\n\n\n","category":"macro"},{"location":"api/#KernelIntrinsics.@fence","page":"API Reference","title":"KernelIntrinsics.@fence","text":"@fence [Scope] [Ordering]\n\nInsert a memory fence with specified scope and ordering.\n\nA memory fence ensures that memory operations before the fence are visible to other threads before operations after the fence. This is essential for correct synchronization in parallel GPU code.\n\nArguments\n\nScope (optional): Visibility scope, one of Device (default), Workgroup, or System\nOrdering (optional): Memory ordering, one of Acquire, Release, AcqRel (default), or SeqCst\n\nArguments can be specified in any order. Weak, Volatile, and Relaxed orderings are not valid for fences.\n\nGenerated PTX\n\n@fence → fence.acq_rel.gpu\n@fence Workgroup → fence.acq_rel.cta\n@fence System SeqCst → fence.sc.sys\n\nExample\n\n@kernel function synchronized_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible before continuing\n    Flag[1] = 1\nend\n\n# Explicit scope and ordering\n@fence Device AcqRel\n@fence Workgroup Release\n@fence System SeqCst\n@fence SeqCst Device  # Order doesn't matter\n\nSee also: @access\n\n\n\n\n\n","category":"macro"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Memory-Fences","page":"Examples","title":"Memory Fences","text":"Use @fence to ensure memory operations are visible across threads before proceeding:\n\nusing KernelIntrinsics\nusing KernelAbstractions, CUDA\n\n@kernel function fence_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible to all threads before next operations\n    Flag[1] = 1\nend\n\nX = cu([1])\nFlag = cu([0])\nfence_kernel(CUDABackend())(X, Flag; ndrange=1)\n\nThe @fence macro generates fence.acq_rel.gpu instructions in PTX assembly, ensuring proper memory ordering across the GPU.","category":"section"},{"location":"examples/#Ordered-Memory-Access","page":"Examples","title":"Ordered Memory Access","text":"The @access macro provides acquire/release semantics for fine-grained memory ordering:\n\n@kernel function access_kernel(X, Flag)\n    if @index(Global, Linear) == 1\n        X[1] = 10\n        @access Flag[1] = 1  # Release store\n    end\n    \n    # Other threads wait for Flag[1] == 1\n    while (@access Acquire Flag[1]) != 1  # Acquire load\n    end\n    \n    # Safely use X[1] here\nend\n\nX = cu([i for i in 1:1000])\nFlag = cu([0])\naccess_kernel(CUDABackend())(X, Flag; ndrange=1000)\n\nThis generates st.release.gpu and ld.acquire.gpu instructions, providing lock-free synchronization patterns.","category":"section"},{"location":"examples/#Warp-Operations","page":"Examples","title":"Warp Operations","text":"","category":"section"},{"location":"examples/#Shuffle-Operations","page":"Examples","title":"Shuffle Operations","text":"Exchange values between threads within a warp:\n\n@kernel function shfl_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    offset = 1\n    shuffled_val = @shfl(Up, val, offset)  # Default: warpsize=32, full mask\n    dst[I] = shuffled_val\nend\n\nsrc = cu([i for i in 1:32])\ndst = cu([0 for i in 1:32])\nshfl_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst = [1, 1, 2, 3, 4, ..., 31]","category":"section"},{"location":"examples/#Shuffle-with-Custom-Types","page":"Examples","title":"Shuffle with Custom Types","text":"Unlike CUDA.jl, KernelIntrinsics.jl supports shuffle operations on arbitrary user-defined bitstype structs, including nested and composite types:\n\nstruct Sub\n    a::Float16\n    b::UInt8\nend\n\nstruct ComplexType\n    x::Int32\n    y::Sub\n    z::Float64\nend\n\n@kernel function shfl_custom_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    offset = 1\n    shuffled_val = @shfl(Up, val, offset)\n    dst[I] = shuffled_val\nend\n\nsrc = cu([ComplexType(i, Sub(i, i), i) for i in 1:32])\ndst = cu([ComplexType(0, Sub(0, 0), 0) for i in 1:32])\nshfl_custom_kernel(CUDABackend())(dst, src; ndrange=32)","category":"section"},{"location":"examples/#Warp-Reduce-(Inclusive-Scan)","page":"Examples","title":"Warp Reduce (Inclusive Scan)","text":"Perform inclusive prefix sum within a warp:\n\n@kernel function warpreduce_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % 32 + 1\n    @warpreduce(val, lane, +)\n    dst[I] = val\nend\n\nsrc = cu([i for i in 1:32])\ndst = cu([0 for i in 1:32])\nwarpreduce_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst = [1, 3, 6, 10, ..., 528]  (cumulative sum)","category":"section"},{"location":"examples/#Warp-Fold-(Reduction)","page":"Examples","title":"Warp Fold (Reduction)","text":"Reduce all values in a warp to a single result:\n\n@kernel function warpfold_kernel(dst, src)\n    I = @index(Global, Linear)\n    val = src[I]\n    lane = (I - 1) % 32 + 1\n    @warpfold(val, lane, +)\n    dst[I] = val\nend\n\nsrc = cu([i for i in 1:32])\ndst = cu([0 for i in 1:32])\nwarpfold_kernel(CUDABackend())(dst, src; ndrange=32)\n# dst[1] = 528 (sum of 1:32), rest are undefined","category":"section"},{"location":"examples/#Vectorized-Memory-Access","page":"Examples","title":"Vectorized Memory Access","text":"Use vectorized loads and stores for improved memory bandwidth:\n\n@kernel function vectorized_kernel(dst, src)\n    values = vload(src, 2, Val(4))  # Load 4 elements starting at index 2\n    vstore!(dst, 2, values)         # Store 4 elements starting at index 2\nend\n\nsrc = cu([Int32(i) for i in 1:32])\ndst = cu([Int32(0) for i in 1:32])\nvectorized_kernel(CUDABackend())(dst, src; ndrange=1)\n# dst = [0, 0, 0, 0, 5, 6, 7, 8, 0, 0, ...]\n\nThis generates efficient ld.global.v4 and st.global.v4 PTX instructions. The vector width depends on element type (v4 for Int32/Float32, v2 for Int64/Float64).","category":"section"},{"location":"examples/#Inspecting-Generated-Code","page":"Examples","title":"Inspecting Generated Code","text":"You can verify the generated PTX assembly to confirm proper instruction generation:\n\nbuf = IOBuffer()\n@device_code_ptx io = buf fence_kernel(CUDABackend())(X, Flag; ndrange=1)\nasm = String(take!(copy(buf)))\noccursin(\"fence.acq_rel.gpu\", asm)  # true","category":"section"},{"location":"#KernelIntrinsics.jl","page":"Home","title":"KernelIntrinsics.jl","text":"A Julia package providing low-level memory access primitives and warp-level operations for GPU programming with KernelAbstractions.jl. KernelIntrinsics.jl enables fine-grained control over memory ordering, synchronization, and vectorized operations for high-performance GPU kernels.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Memory Fences: Explicit memory synchronization with @fence macro\nOrdered Memory Access: Acquire/release semantics with @access macro\nWarp Operations: Efficient intra-warp communication and reduction primitives\n@shfl: Warp shuffle operations (Up, Down, Xor, Idx modes)\n@warpreduce: Inclusive scan within a warp\n@warpfold: Warp-wide reduction to a single value\nVectorized Memory Operations: Hardware-accelerated vector loads and stores with vload and vstore!","category":"section"},{"location":"#Cross-Architecture-Support","page":"Home","title":"Cross-Architecture Support","text":"Currently, KernelIntrinsics.jl is implemented exclusively for CUDA GPUs via the CUDABackend. The package leverages CUDA-specific PTX instructions for memory fences (fence.acq_rel.gpu), ordered memory access (ld.acquire.gpu, st.release.gpu), warp shuffle operations, and vectorized memory transactions.\n\nWhile the current implementation is CUDA-specific, the macro-based API is designed with portability in mind. Future releases may extend support to other GPU backends (AMD ROCm, Intel oneAPI, Apple Metal). Contributions to enable cross-platform support are welcome.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"KernelIntrinsics\")","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KernelIntrinsics\nusing KernelAbstractions, CUDA\n\n@kernel function example_kernel(X, Flag)\n    X[1] = 10\n    @fence  # Ensure X[1]=10 is visible to all threads\n    @access Flag[1] = 1  # Release store\nend\n\nX = cu([1])\nFlag = cu([0])\nexample_kernel(CUDABackend())(X, Flag; ndrange=1)","category":"section"},{"location":"#Memory-Ordering-Semantics","page":"Home","title":"Memory Ordering Semantics","text":"@fence: Full acquire-release fence across all device threads\n@access Release: Ensures prior writes are visible before the store\n@access Acquire: Ensures subsequent reads see prior writes","category":"section"},{"location":"#Performance-Considerations","page":"Home","title":"Performance Considerations","text":"Warp operations are most efficient when all threads in a warp participate\nVectorized operations can significantly improve memory bandwidth utilization\nUse the minimum required memory ordering (acquire/release over fences when possible)\nDefault warp size is 32; operations assume full warp participation","category":"section"},{"location":"#Implementation-Notes","page":"Home","title":"Implementation Notes","text":"","category":"section"},{"location":"#Memory-Ordering-and-Scopes","page":"Home","title":"Memory Ordering and Scopes","text":"The implementation of memory fences, orderings, and scopes is inspired by UnsafeAtomics.jl. This package includes tests demonstrating that these primitives work correctly on CUDA, generating the expected PTX instructions (fence.acq_rel.gpu, ld.acquire.gpu, st.release.gpu, etc.).","category":"section"},{"location":"#Warp-Shuffle-Operations","page":"Home","title":"Warp Shuffle Operations","text":"The warp shuffle implementation builds upon CUDA.jl's approach but generalizes it to support any concrete bitstype struct, including nested and composite types. The backend only needs to implement 32-bit shuffle operations; larger types are automatically decomposed. Future backends that do not natively support 32-bit atomic operations may require additional handling.","category":"section"},{"location":"#Vectorized-Memory-Access","page":"Home","title":"Vectorized Memory Access","text":"Vectorized loads and stores (vload, vstore!) use LLVM intrinsic functions to generate efficient vector instructions (ld.global.v4, st.global.v4). \n\nCurrent limitations:\n\nArrays must be contiguous in memory\nViews are not supported\nFuture versions may add fallback paths using tuples for non-contiguous access","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.6+\nKernelAbstractions.jl\nCUDA.jl (for CUDA backend)","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"api.md\", \"examples.md\"]\nDepth = 2","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"}]
}
